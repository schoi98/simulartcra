{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simulartcra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "logging.basicConfig(level=logging.ERROR)\n",
    "\n",
    "from datetime import datetime, timedelta\n",
    "from typing import List\n",
    "import collections\n",
    "import inspect\n",
    "import tenacity\n",
    "from termcolor import colored\n",
    "import os\n",
    "import io\n",
    "import warnings\n",
    "from PIL import Image\n",
    "from stability_sdk import client\n",
    "import stability_sdk.interfaces.gooseai.generation.generation_pb2 as generation\n",
    "\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.docstore import InMemoryDocstore\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.retrievers import TimeWeightedVectorStoreRetriever\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.schema import (\n",
    "    HumanMessage,\n",
    "    SystemMessage,\n",
    ")\n",
    "from langchain.output_parsers import RegexParser\n",
    "from langchain_experimental.generative_agents import (\n",
    "    GenerativeAgent,\n",
    "    GenerativeAgentMemory,\n",
    ")\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "os.environ['STABILITY_HOST'] = 'grpc.stability.ai:443'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:stability_sdk.client:Opening channel to grpc.stability.ai:443\n",
      "INFO:stability_sdk.client:Channel opened to grpc.stability.ai:443\n"
     ]
    }
   ],
   "source": [
    "USER_NAME = \"schoi\"  # The name you want to use when interviewing the agent.\n",
    "LLM = ChatOpenAI(max_tokens=1500)  # Can be any LLM you want.\n",
    "# Set up our connection to the API.\n",
    "stability_api = client.StabilityInference(\n",
    "    key=os.environ['STABILITY_KEY'], # API Key reference.\n",
    "    verbose=True, # Print debug messages.\n",
    "    engine=\"stable-diffusion-xl-1024-v1-0\", # Set the engine to use for generation.\n",
    "    # Check out the following link for a list of available engines: https://platform.stability.ai/docs/features/api-parameters#engine\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class GymnasiumGenerativeAgent:\n",
    "    @classmethod\n",
    "    def get_docs(cls, env):\n",
    "        return env.unwrapped.__doc__\n",
    "\n",
    "    def __init__(self, agent, env):\n",
    "        self.agent = agent\n",
    "        self.env = env\n",
    "        self.docs = self.get_docs(env)\n",
    "\n",
    "        self.instructions = \"\"\"\n",
    "Your goal is to maximize your return, i.e. the sum of the rewards you receive.\n",
    "I will give you an observation, reward, terminiation flag, truncation flag, and the return so far, formatted as:\n",
    "\n",
    "Observation: <observation>\n",
    "Reward: <reward>\n",
    "Termination: <termination>\n",
    "Truncation: <truncation>\n",
    "Return: <sum_of_rewards>\n",
    "\n",
    "You will respond with an action, formatted as:\n",
    "\n",
    "Action: <action>\n",
    "\n",
    "where you replace <action> with your actual action.\n",
    "Do nothing else but return the action.\n",
    "\"\"\"\n",
    "        self.action_parser = RegexParser(\n",
    "            regex=r\"Action: (.*)\", output_keys=[\"action\"], default_output_key=\"action\"\n",
    "        )\n",
    "\n",
    "        self.message_history = []\n",
    "        self.ret = 0\n",
    "\n",
    "    def random_action(self):\n",
    "        action = self.env.action_space.sample()\n",
    "        return action\n",
    "\n",
    "    def reset(self):\n",
    "        self.message_history = [\n",
    "            SystemMessage(content=self.docs),\n",
    "            SystemMessage(content=self.instructions),\n",
    "        ]\n",
    "\n",
    "    def observe(self, obs, rew=0, term=False, trunc=False, info=None):\n",
    "        self.ret += rew\n",
    "\n",
    "        obs_message = f\"\"\"\n",
    "Observation: {obs}\n",
    "Reward: {rew}\n",
    "Termination: {term}\n",
    "Truncation: {trunc}\n",
    "Return: {self.ret}\n",
    "        \"\"\"\n",
    "        self.message_history.append(HumanMessage(content=obs_message))\n",
    "        return obs_message\n",
    "\n",
    "    def _act(self):\n",
    "        act_message = self.model(self.message_history)\n",
    "        self.message_history.append(act_message)\n",
    "        action = int(self.action_parser.parse(act_message.content)[\"action\"])\n",
    "        return action\n",
    "\n",
    "    def act(self):\n",
    "        try:\n",
    "            for attempt in tenacity.Retrying(\n",
    "                stop=tenacity.stop_after_attempt(2),\n",
    "                wait=tenacity.wait_none(),  # No waiting time between retries\n",
    "                retry=tenacity.retry_if_exception_type(ValueError),\n",
    "                before_sleep=lambda retry_state: print(\n",
    "                    f\"ValueError occurred: {retry_state.outcome.exception()}, retrying...\"\n",
    "                ),\n",
    "            ):\n",
    "                with attempt:\n",
    "                    action = self._act()\n",
    "        except tenacity.RetryError as e:\n",
    "            action = self.random_action()\n",
    "        return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PettingZooGenerativeAgent(GymnasiumGenerativeAgent):\n",
    "    @classmethod\n",
    "    def get_docs(cls, env):\n",
    "        return inspect.getmodule(env.unwrapped).__doc__\n",
    "\n",
    "    def __init__(self, name, agent, env):\n",
    "        super().__init__(agent, env)\n",
    "        self.name = name\n",
    "\n",
    "    def random_action(self):\n",
    "        action = self.env.action_space(self.name).sample()\n",
    "        return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActionMaskGenerativeAgent(PettingZooGenerativeAgent):\n",
    "    def __init__(self, name, agent, env):\n",
    "        super().__init__(name, agent, env)\n",
    "        self.obs_buffer = collections.deque(maxlen=1)\n",
    "\n",
    "    def random_action(self):\n",
    "        obs = self.obs_buffer[-1]\n",
    "        action = self.env.action_space(self.name).sample(obs[\"action_mask\"])\n",
    "        return action\n",
    "\n",
    "    def reset(self):\n",
    "        self.message_history = [\n",
    "            SystemMessage(content=self.docs),\n",
    "            SystemMessage(content=self.instructions),\n",
    "        ]\n",
    "\n",
    "    def observe(self, obs, rew=0, term=False, trunc=False, info=None):\n",
    "        self.obs_buffer.append(obs)\n",
    "        return super().observe(obs, rew, term, trunc, info)\n",
    "\n",
    "    def _act(self):\n",
    "        valid_action_instruction = \"Generate a valid action given by the indices of the `action_mask` that are not 0, according to the action formatting rules.\"\n",
    "        self.message_history.append(HumanMessage(content=valid_action_instruction))\n",
    "        return super()._act()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import faiss\n",
    "\n",
    "\n",
    "def relevance_score_fn(score: float) -> float:\n",
    "    \"\"\"Return a similarity score on a scale [0, 1].\"\"\"\n",
    "    # This will differ depending on a few things:\n",
    "    # - the distance / similarity metric used by the VectorStore\n",
    "    # - the scale of your embeddings (OpenAI's are unit norm. Many others are not!)\n",
    "    # This function converts the euclidean norm of normalized embeddings\n",
    "    # (0 is most similar, sqrt(2) most dissimilar)\n",
    "    # to a similarity function (0 to 1)\n",
    "    return 1.0 - score / math.sqrt(2)\n",
    "\n",
    "\n",
    "def create_new_memory_retriever():\n",
    "    \"\"\"Create a new vector store retriever unique to the agent.\"\"\"\n",
    "    # Define your embedding model\n",
    "    embeddings_model = OpenAIEmbeddings()\n",
    "    # Initialize the vectorstore as empty\n",
    "    embedding_size = 1536\n",
    "    index = faiss.IndexFlatL2(embedding_size)\n",
    "    vectorstore = FAISS(\n",
    "        embeddings_model.embed_query,\n",
    "        index,\n",
    "        InMemoryDocstore({}),\n",
    "        {},\n",
    "        relevance_score_fn=relevance_score_fn,\n",
    "    )\n",
    "    return TimeWeightedVectorStoreRetriever(\n",
    "        vectorstore=vectorstore, other_score_keys=[\"importance\"], k=15\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "## stability ai art\n",
    "def generate_art(prompt, seed=4253978046, init_img=None, mask=None, samples=1, steps=50, cfg_scale=8.0, width=1024, height=1024, sampler=generation.SAMPLER_K_DPMPP_2M):\n",
    "    answers = stability_api.generate(\n",
    "        prompt=prompt,\n",
    "        init_image=init_img, # Assign our previously generated img as our Initial Image for transformation.\n",
    "        mask_image=mask,\n",
    "        seed=seed, # If a seed is provided, the resulting generated image will be deterministic.\n",
    "                        # What this means is that as long as all generation parameters remain the same, you can always recall the same image simply by generating it again.\n",
    "                        # Note: This isn't quite the case for Clip Guided generations, which we'll tackle in a future example notebook.\n",
    "        steps=steps, # Amount of inference steps performed on image generation. Defaults to 30. \n",
    "        cfg_scale=cfg_scale, # Influences how strongly your generation is guided to match your prompt.\n",
    "        width=width, # Generation width, defaults to 512 if not included.\n",
    "        height=height, # Generation height, defaults to 512 if not included.\n",
    "        samples=samples, # Number of images to generate, defaults to 1 if not included.\n",
    "        sampler=sampler\n",
    "    )\n",
    "    for resp in answers:\n",
    "        for artifact in resp.artifacts:\n",
    "            if artifact.finish_reason == generation.FILTER:\n",
    "                warnings.warn(\n",
    "                    \"Your request activated the API's safety filters and could not be processed.\"\n",
    "                    \"Please modify the prompt and try again.\")\n",
    "            if artifact.type == generation.ARTIFACT_IMAGE:\n",
    "                img = Image.open(io.BytesIO(artifact.binary))\n",
    "                img.save(str(artifact.seed)+ \".png\") # Save our generated images with their seed number as the filename."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alices_memory = GenerativeAgentMemory(\n",
    "    llm=LLM,\n",
    "    memory_retriever=create_new_memory_retriever(),\n",
    "    verbose=False,\n",
    "    reflection_threshold=8,  # we will give this a relatively low number to show how reflection works\n",
    ")\n",
    "\n",
    "alice = GenerativeAgent(\n",
    "    name=\"Alice\",\n",
    "    age=25,\n",
    "    traits=\"expressive, likes classical art\",  # You can add more persistent traits here\n",
    "    status=\"wants to create art\",  # When connected to a virtual world, we can have the characters update their status\n",
    "    memory_retriever=create_new_memory_retriever(),\n",
    "    llm=LLM,\n",
    "    memory=alices_memory,\n",
    ")\n",
    "\n",
    "bobs_memory = GenerativeAgentMemory(\n",
    "    llm=LLM,\n",
    "    memory_retriever=create_new_memory_retriever(),\n",
    "    verbose=False,\n",
    "    reflection_threshold=8,  # we will give this a relatively low number to show how reflection works\n",
    ")\n",
    "\n",
    "bob = GenerativeAgent(\n",
    "    name=\"Bob\",\n",
    "    age=25,\n",
    "    traits=\"bold, likes modern art\",  # You can add more persistent traits here\n",
    "    status=\"wants to create art\",  # When connected to a virtual world, we can have the characters update their status\n",
    "    memory_retriever=create_new_memory_retriever(),\n",
    "    llm=LLM,\n",
    "    memory=bobs_memory,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can add memories directly to the memory object\n",
    "alices_observation = [\n",
    "    \"Alice is going to create art\", \n",
    "    \"Alice remembers having to collaborate with Bob\",\n",
    "    \"Alice wants to come to a consensus with Bob\"\n",
    "]\n",
    "bobs_observation = [\n",
    "    \"Bob is going to create art\",\n",
    "    \"Bob remembers having to collaborate with Alice\",\n",
    "    \"Bob wants to come to a consensus with Alice\"\n",
    "]\n",
    "\n",
    "for observation in alices_observation:\n",
    "    alice.memory.add_memory(observation)\n",
    "for observation in bobs_observation:\n",
    "    bob.memory.add_memory(observation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def interview_agent(agent: GenerativeAgent, message: str) -> str:\n",
    "    \"\"\"Help the notebook user interact with the agent.\"\"\"\n",
    "    new_message = f\"{USER_NAME} says {message}\"\n",
    "    return agent.generate_dialogue_response(new_message)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interview_agent(alice, \"What do you like to do?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interview_agent(bob, \"What do you like to do?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env=\"env\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agents = {\n",
    "    name: ActionMaskGenerativeAgent(name=name, agent=agent, env=env)\n",
    "    for agent in [alice, bob]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main Event Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(agents, env):\n",
    "    env.reset()\n",
    "\n",
    "    for name, agent in agents.items():\n",
    "        print(name)\n",
    "        agent.reset()\n",
    "\n",
    "    for agent_name in env.agent_iter():\n",
    "        print(agent_name)\n",
    "        observation, reward, termination, truncation, info = env.last()\n",
    "        obs_message = agents[agent_name].observe(\n",
    "            observation, reward, termination, truncation, info\n",
    "        )\n",
    "        print(obs_message)\n",
    "        if termination or truncation:\n",
    "            action = None\n",
    "        else:\n",
    "            action = agents[agent_name].act()\n",
    "        print(f\"Action: {action}\")\n",
    "        env.step(action)\n",
    "    env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main(agents, env)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ishb",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
