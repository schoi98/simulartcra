{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simulartcra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "logging.basicConfig(level=logging.ERROR)\n",
    "\n",
    "from datetime import datetime, timedelta\n",
    "from typing import List\n",
    "import collections\n",
    "import inspect\n",
    "import tenacity\n",
    "from termcolor import colored\n",
    "\n",
    "\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.docstore import InMemoryDocstore\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.retrievers import TimeWeightedVectorStoreRetriever\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.schema import (\n",
    "    HumanMessage,\n",
    "    SystemMessage,\n",
    ")\n",
    "from langchain.output_parsers import RegexParser\n",
    "from langchain_experimental.generative_agents import (\n",
    "    GenerativeAgent,\n",
    "    GenerativeAgentMemory,\n",
    ")\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "USER_NAME = \"schoi\"  # The name you want to use when interviewing the agent.\n",
    "LLM = ChatOpenAI(max_tokens=1500)  # Can be any LLM you want."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GymnasiumAgent:\n",
    "    @classmethod\n",
    "    def get_docs(cls, env):\n",
    "        return env.unwrapped.__doc__\n",
    "\n",
    "    def __init__(self, model, env):\n",
    "        self.model = model\n",
    "        self.env = env\n",
    "        self.docs = self.get_docs(env)\n",
    "\n",
    "        self.instructions = \"\"\"\n",
    "Your goal is to maximize your return, i.e. the sum of the rewards you receive.\n",
    "I will give you an observation, reward, terminiation flag, truncation flag, and the return so far, formatted as:\n",
    "\n",
    "Observation: <observation>\n",
    "Reward: <reward>\n",
    "Termination: <termination>\n",
    "Truncation: <truncation>\n",
    "Return: <sum_of_rewards>\n",
    "\n",
    "You will respond with an action, formatted as:\n",
    "\n",
    "Action: <action>\n",
    "\n",
    "where you replace <action> with your actual action.\n",
    "Do nothing else but return the action.\n",
    "\"\"\"\n",
    "        self.action_parser = RegexParser(\n",
    "            regex=r\"Action: (.*)\", output_keys=[\"action\"], default_output_key=\"action\"\n",
    "        )\n",
    "\n",
    "        self.message_history = []\n",
    "        self.ret = 0\n",
    "\n",
    "    def random_action(self):\n",
    "        action = self.env.action_space.sample()\n",
    "        return action\n",
    "\n",
    "    def reset(self):\n",
    "        self.message_history = [\n",
    "            SystemMessage(content=self.docs),\n",
    "            SystemMessage(content=self.instructions),\n",
    "        ]\n",
    "\n",
    "    def observe(self, obs, rew=0, term=False, trunc=False, info=None):\n",
    "        self.ret += rew\n",
    "\n",
    "        obs_message = f\"\"\"\n",
    "Observation: {obs}\n",
    "Reward: {rew}\n",
    "Termination: {term}\n",
    "Truncation: {trunc}\n",
    "Return: {self.ret}\n",
    "        \"\"\"\n",
    "        self.message_history.append(HumanMessage(content=obs_message))\n",
    "        return obs_message\n",
    "\n",
    "    def _act(self):\n",
    "        act_message = self.model(self.message_history)\n",
    "        self.message_history.append(act_message)\n",
    "        action = int(self.action_parser.parse(act_message.content)[\"action\"])\n",
    "        return action\n",
    "\n",
    "    def act(self):\n",
    "        try:\n",
    "            for attempt in tenacity.Retrying(\n",
    "                stop=tenacity.stop_after_attempt(2),\n",
    "                wait=tenacity.wait_none(),  # No waiting time between retries\n",
    "                retry=tenacity.retry_if_exception_type(ValueError),\n",
    "                before_sleep=lambda retry_state: print(\n",
    "                    f\"ValueError occurred: {retry_state.outcome.exception()}, retrying...\"\n",
    "                ),\n",
    "            ):\n",
    "                with attempt:\n",
    "                    action = self._act()\n",
    "        except tenacity.RetryError as e:\n",
    "            action = self.random_action()\n",
    "        return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PettingZooAgent(GymnasiumAgent):\n",
    "    @classmethod\n",
    "    def get_docs(cls, env):\n",
    "        return inspect.getmodule(env.unwrapped).__doc__\n",
    "\n",
    "    def __init__(self, name, model, env):\n",
    "        super().__init__(model, env)\n",
    "        self.name = name\n",
    "\n",
    "    def random_action(self):\n",
    "        action = self.env.action_space(self.name).sample()\n",
    "        return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActionMaskAgent(PettingZooAgent):\n",
    "    def __init__(self, name, model, env):\n",
    "        super().__init__(name, model, env)\n",
    "        self.obs_buffer = collections.deque(maxlen=1)\n",
    "\n",
    "    def random_action(self):\n",
    "        obs = self.obs_buffer[-1]\n",
    "        action = self.env.action_space(self.name).sample(obs[\"action_mask\"])\n",
    "        return action\n",
    "\n",
    "    def reset(self):\n",
    "        self.message_history = [\n",
    "            SystemMessage(content=self.docs),\n",
    "            SystemMessage(content=self.instructions),\n",
    "        ]\n",
    "\n",
    "    def observe(self, obs, rew=0, term=False, trunc=False, info=None):\n",
    "        self.obs_buffer.append(obs)\n",
    "        return super().observe(obs, rew, term, trunc, info)\n",
    "\n",
    "    def _act(self):\n",
    "        valid_action_instruction = \"Generate a valid action given by the indices of the `action_mask` that are not 0, according to the action formatting rules.\"\n",
    "        self.message_history.append(HumanMessage(content=valid_action_instruction))\n",
    "        return super()._act()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import faiss\n",
    "\n",
    "\n",
    "def relevance_score_fn(score: float) -> float:\n",
    "    \"\"\"Return a similarity score on a scale [0, 1].\"\"\"\n",
    "    # This will differ depending on a few things:\n",
    "    # - the distance / similarity metric used by the VectorStore\n",
    "    # - the scale of your embeddings (OpenAI's are unit norm. Many others are not!)\n",
    "    # This function converts the euclidean norm of normalized embeddings\n",
    "    # (0 is most similar, sqrt(2) most dissimilar)\n",
    "    # to a similarity function (0 to 1)\n",
    "    return 1.0 - score / math.sqrt(2)\n",
    "\n",
    "\n",
    "def create_new_memory_retriever():\n",
    "    \"\"\"Create a new vector store retriever unique to the agent.\"\"\"\n",
    "    # Define your embedding model\n",
    "    embeddings_model = OpenAIEmbeddings()\n",
    "    # Initialize the vectorstore as empty\n",
    "    embedding_size = 1536\n",
    "    index = faiss.IndexFlatL2(embedding_size)\n",
    "    vectorstore = FAISS(\n",
    "        embeddings_model.embed_query,\n",
    "        index,\n",
    "        InMemoryDocstore({}),\n",
    "        {},\n",
    "        relevance_score_fn=relevance_score_fn,\n",
    "    )\n",
    "    return TimeWeightedVectorStoreRetriever(\n",
    "        vectorstore=vectorstore, other_score_keys=[\"importance\"], k=15\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## stability ai art\n",
    "def generate_art_from_text():\n",
    "    pass\n",
    "\n",
    "def generate_art_from_image():\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alices_memory = GenerativeAgentMemory(\n",
    "    llm=LLM,\n",
    "    memory_retriever=create_new_memory_retriever(),\n",
    "    verbose=False,\n",
    "    reflection_threshold=8,  # we will give this a relatively low number to show how reflection works\n",
    ")\n",
    "\n",
    "alice = GenerativeAgent(\n",
    "    name=\"Alice\",\n",
    "    age=25,\n",
    "    traits=\"expressive, likes classical art\",  # You can add more persistent traits here\n",
    "    status=\"wants to create art\",  # When connected to a virtual world, we can have the characters update their status\n",
    "    memory_retriever=create_new_memory_retriever(),\n",
    "    llm=LLM,\n",
    "    memory=alices_memory,\n",
    ")\n",
    "\n",
    "bobs_memory = GenerativeAgentMemory(\n",
    "    llm=LLM,\n",
    "    memory_retriever=create_new_memory_retriever(),\n",
    "    verbose=False,\n",
    "    reflection_threshold=8,  # we will give this a relatively low number to show how reflection works\n",
    ")\n",
    "\n",
    "bob = GenerativeAgent(\n",
    "    name=\"Bob\",\n",
    "    age=25,\n",
    "    traits=\"bold, likes modern art\",  # You can add more persistent traits here\n",
    "    status=\"wants to create art\",  # When connected to a virtual world, we can have the characters update their status\n",
    "    memory_retriever=create_new_memory_retriever(),\n",
    "    llm=LLM,\n",
    "    memory=bobs_memory,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can add memories directly to the memory object\n",
    "alices_observation = [\n",
    "    \"Alice is going to create art\", \n",
    "    \"Alice remembers having to collaborate with Bob\",\n",
    "    \"Alice wants to come to a consensus with Bob\"\n",
    "]\n",
    "bobs_observation = [\n",
    "    \"Bob is going to create art\",\n",
    "    \"Bob remembers having to collaborate with Alice\",\n",
    "    \"Bob wants to come to a consensus with Alice\"\n",
    "]\n",
    "\n",
    "for observation in alices_observation:\n",
    "    alice.memory.add_memory(observation)\n",
    "for observation in bobs_observation:\n",
    "    bob.memory.add_memory(observation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def interview_agent(agent: GenerativeAgent, message: str) -> str:\n",
    "    \"\"\"Help the notebook user interact with the agent.\"\"\"\n",
    "    new_message = f\"{USER_NAME} says {message}\"\n",
    "    return agent.generate_dialogue_response(new_message)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interview_agent(alice, \"What do you like to do?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interview_agent(bob, \"What do you like to do?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main Event Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(agents, env):\n",
    "    env.reset()\n",
    "\n",
    "    for name, agent in agents.items():\n",
    "        agent.reset()\n",
    "\n",
    "    for agent_name in env.agent_iter():\n",
    "        observation, reward, termination, truncation, info = env.last()\n",
    "        obs_message = agents[agent_name].observe(\n",
    "            observation, reward, termination, truncation, info\n",
    "        )\n",
    "        print(obs_message)\n",
    "        if termination or truncation:\n",
    "            action = None\n",
    "        else:\n",
    "            action = agents[agent_name].act()\n",
    "        print(f\"Action: {action}\")\n",
    "        env.step(action)\n",
    "    env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pettingzoo.classic import tictactoe_v3\n",
    "\n",
    "env = tictactoe_v3.env(render_mode=\"human\")\n",
    "agents = {\n",
    "    name: ActionMaskAgent(name=name, model=ChatOpenAI(temperature=0.2), env=env)\n",
    "    for name in env.possible_agents\n",
    "}\n",
    "main(agents, env)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ishb",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
